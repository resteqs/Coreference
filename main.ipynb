{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manto\\anaconda3\\envs\\aligner\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import whisper\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import time\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove symbols and special characters from text.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s\\']', '', text)\n",
    "\n",
    "def convert_conll_to_txt(conll_file, output_dir, base_name):\n",
    "    \"\"\"Convert a CoNLL file to a cleaned text file.\n",
    "\n",
    "    - Extracts words from the CoNLL file.\n",
    "    - Cleans the text by removing unwanted characters and extra spaces.\n",
    "    - Writes the cleaned text to an output file.\n",
    "\n",
    "    Args:\n",
    "        conll_file (str): Path to the input CoNLL file.\n",
    "        txt_file \n",
    "    \"\"\"\n",
    "    txt_file = os.path.join(output_dir, f\"{base_name}.txt\")\n",
    "    words = []\n",
    "    with open(conll_file, 'r', encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            if line.strip():  # Non-empty line\n",
    "                tokens = line.strip().split()\n",
    "                if len(tokens) > 3:  # Ensure enough columns\n",
    "                    word = clean_text(tokens[3])  # Clean the word\n",
    "                    if word:  # Only add non-empty words\n",
    "                        words.append(word)\n",
    "            else:\n",
    "                # Add sentence boundary\n",
    "                if words:\n",
    "                    words.append('\\n')\n",
    "    \n",
    "    # Write cleaned text to output file\n",
    "    with open(txt_file, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(' '.join(words))\n",
    "    return txt_file\n",
    "\n",
    "\n",
    "def run_mfa_align(input_dir, output_dir):\n",
    "    \"\"\"\"Run the Montreal Forced Aligner (MFA) on the input files.\n",
    "\n",
    "    - Aligns the audio and text files in the input directory.\n",
    "    - Generates a TextGrid file with alignment data.\n",
    "    - Saves the TextGrid file in the output directory.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing input files.\n",
    "        output_dir (str): Path to the directory where output files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure directories exist\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"Input directory does not exist: {input_dir}\")\n",
    "        return\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Construct the MFA command\n",
    "    command = [\n",
    "        'mfa', 'align',\n",
    "        input_dir,\n",
    "        'english_mfa',  # Specify the pronunciation dictionary\n",
    "        'english_mfa',  # Specify the acoustic model\n",
    "        output_dir\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"Alignment completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error during alignment: {e}\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def extract_textgrid_intervals(textgrid_path):\n",
    "    \"\"\"Extract word intervals from a TextGrid file.\n",
    "\n",
    "    - Parses the TextGrid file to extract words and their timing intervals.\n",
    "    - Collects intervals where words are present and skips empty intervals.\n",
    "\n",
    "    Args:\n",
    "        textgrid_file (str): Path to the TextGrid file.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with the following keys:\n",
    "            - 'word' (str): The word from the interval.\n",
    "            - 'start_time' (float): Start time of the interval.\n",
    "            - 'end_time' (float): End time of the interval.\n",
    "    \"\"\"\n",
    "    word_intervals = []\n",
    "    with open(textgrid_path, 'r' ,encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    is_word_tier = False\n",
    "    for i, line in enumerate(lines):\n",
    "        if 'name = \"words\"' in line:\n",
    "            is_word_tier = True\n",
    "            continue\n",
    "        \n",
    "        # Extract interval data within the words tier with some regex magic\n",
    "        if is_word_tier:\n",
    "            if 'xmin' in line:\n",
    "                xmin = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "            elif 'xmax' in line:\n",
    "                xmax = float(re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", line).group())\n",
    "            elif 'text' in line:\n",
    "                text_match = re.search(r'text = \"(.*)\"', line)\n",
    "                text = text_match.group(1).strip() if text_match else \"\"\n",
    "                \n",
    "                # We skip empty text\n",
    "                if text:\n",
    "                    word_intervals.append({\n",
    "                        \"word\": text,\n",
    "                        \"start_time\": xmin,\n",
    "                        \"end_time\": xmax\n",
    "                    })\n",
    "            \n",
    "            if 'name' in line and 'name = ' in line and not 'name = \"words\"' in line:\n",
    "                break\n",
    "    \n",
    "    return word_intervals\n",
    "\n",
    "\n",
    "def verify_mfa_alignment_whisper(audio_file, intervals_from_TxtGrid, model):\n",
    "    \"\"\"Verify MFA alignment using Whisper and return results in a DataFrame.\n",
    "\n",
    "    - Loads a Whisper model for transcription.\n",
    "    - Segments the audio file based on intervals from the TextGrid.\n",
    "    - Transcribes each segment and compares it with the expected word. I checked and whisper fills in the short files with silence to match the correct input shape. \n",
    "     This should not cause any issues\n",
    "    - Saves the results, including execution time and timestamps.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file (.wav).\n",
    "        intervals_from_TxtGrid (list): Intervals with words and their timings.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Contains columns:\n",
    "            - 'Correct Word': The expected word from the interval.\n",
    "            - 'Whisper Prediction': The word predicted by Whisper.\n",
    "            - 'Match': 1 if the words match, 0 otherwise.\n",
    "            - 'Start Time': Start time of the interval.\n",
    "            - 'End Time': End time of the interval.\n",
    "            - 'Execution Time': Cumulative time taken up to this point.\n",
    "    \"\"\"\n",
    "    print(\"Loading Whisper model...\")\n",
    "    model = whisper.load_model(model) #--------------------------tiny, base, medium, large, turbo---------------------------------\n",
    "    \n",
    "    # Load audio file\n",
    "    print(f\"Loading audio file: {audio_file}\")\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "    \n",
    "    total_words = len(intervals_from_TxtGrid)\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for index, interval in enumerate(intervals_from_TxtGrid, 1):\n",
    "        word = interval[\"word\"]\n",
    "        if not word or word.isspace():\n",
    "            continue\n",
    "            \n",
    "        # Extract time segment\n",
    "        start_ms = int(interval[\"start_time\"] * 1000)\n",
    "        end_ms = int(interval[\"end_time\"] * 1000)\n",
    "        segment = audio[start_ms:end_ms]\n",
    "        \n",
    "        # Process segment\n",
    "        temp_file = \"temp_segment.wav\"\n",
    "        segment.export(temp_file, format=\"wav\")\n",
    "        \n",
    "        # Get Whisper transcription\n",
    "        result = model.transcribe(temp_file)\n",
    "        whisper_text = result[\"text\"].strip().lower()\n",
    "        \n",
    "        # Compare results\n",
    "        mfa_word = word.lower().strip()\n",
    "        is_match = 1 if mfa_word == whisper_text else 0\n",
    "        \n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        # Append to results list\n",
    "        results.append({\n",
    "            'Correct Word': mfa_word,\n",
    "            'Whisper Prediction': whisper_text,\n",
    "            'Match': is_match,\n",
    "            'Start Time': f\"{interval['start_time']:.2f}\",\n",
    "            'End Time': f\"{interval['end_time']:.2f}\",\n",
    "            'Execution Time': f\"{exec_time:.2f}\"\n",
    "        })\n",
    "        \n",
    "        # Remove temp file\n",
    "        os.remove(temp_file)\n",
    "        \n",
    "        # Print progress\n",
    "        progress = (index / total_words) * 100\n",
    "        print(f\"{index}/{total_words} ; {progress:.2f}%\")\n",
    "    \n",
    "    # Create a pandas DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def align_sequences(conll_words, textgrid_intervals): #Using the Needleman-Wunsch algortihm for alligning sequences, as the files don't fit perfectly\n",
    "    \"\"\"Aligns words from the CoNLL file with words from the TextGrid intervals using the *Needleman-Wunsch* algorithm.\n",
    "\n",
    "    Steps:\n",
    "    1. **Initialization:**\n",
    "       - Create a scoring matrix (`score_matrix`) of size (n+1) x (m+1), where n is the length of `conll_words` and m is the length of `textgrid_words`.\n",
    "       - Initialize the first row and column with cumulative gap penalties to represent alignments with leading gaps.\n",
    "\n",
    "    2. **Scoring Matrix Construction:**\n",
    "       - Iterate over each cell in the matrix to compute the optimal score based on:\n",
    "         - **Match:** If the current words from `conll_words` and `textgrid_words` match, add a positive `match_score`.\n",
    "         - **Mismatch:** If the words do not match, add a negative `mismatch_penalty`.\n",
    "         - **Insertion/Deletion (Gaps):** Assign a negative `gap_penalty` for insertions or deletions.\n",
    "       - Calculate scores for:\n",
    "         - **Diagonal move (match/mismatch):** `score_matrix[i-1][j-1] + score`\n",
    "         - **Up move (deletion in TextGrid):** `score_matrix[i-1][j] + gap_penalty`\n",
    "         - **Left move (insertion in TextGrid):** `score_matrix[i][j-1] + gap_penalty`\n",
    "       - Select the move with the highest score and store it in `score_matrix[i][j]`.\n",
    "       - Record the move direction in the `traceback_matrix` for later path reconstruction.\n",
    "\n",
    "    3. **Traceback:**\n",
    "       - Start from the bottom-right cell of the matrices (`score_matrix[n][m]`).\n",
    "       - Reconstruct the optimal alignment by moving in the direction indicated by `traceback_matrix`:\n",
    "         - **'diag':** Align the current words from both sequences.\n",
    "         - **'up':** Align the word from `conll_words` with a gap (deletion in `textgrid_words`).\n",
    "         - **'left':** Align a gap with the word from `textgrid_words` (insertion in `conll_words`).\n",
    "       - Continue tracing back until reaching the top-left cell.\n",
    "\n",
    "    4. **Results:**\n",
    "       - Obtain `aligned_conll` and `aligned_textgrid`, which are the aligned sequences including gaps ('-') where necessary.\n",
    "       - Collect corresponding time intervals from `textgrid_intervals` for aligned words.\n",
    "       - Reverse the aligned sequences and times to obtain the correct order.\n",
    "\n",
    "    Parameters:\n",
    "        conll_words (list): List of words extracted from the CoNLL file.\n",
    "        textgrid_intervals (list): List of dictionaries from the TextGrid file, each containing:\n",
    "                                   - \"word\": The word string.\n",
    "                                   - \"start_time\": Start time of the interval.\n",
    "                                   - \"end_time\": End time of the interval.\n",
    "\n",
    "    Returns:\n",
    "        aligned_conll (list): Aligned words from the CoNLL file, including gaps.\n",
    "        aligned_textgrid (list): Aligned words from the TextGrid intervals, including gaps.\n",
    "        aligned_times (list): List of (start_time, end_time) tuples corresponding to `aligned_textgrid`.\"\"\"\n",
    "\n",
    "\n",
    "    # lets get Textgrid words and tinestamps\n",
    "    textgrid_words = [str(interval[\"word\"]) for interval in textgrid_intervals]\n",
    "    textgrid_times = [(interval[\"start_time\"], interval[\"end_time\"]) for interval in textgrid_intervals]\n",
    "\n",
    "    # Initialize scoring parameters\n",
    "    match_score = 2\n",
    "    mismatch_penalty = -1\n",
    "    gap_penalty = -2\n",
    "\n",
    "    n = len(conll_words)\n",
    "    m = len(textgrid_words)\n",
    "\n",
    "    # Initialize the scoring matrix\n",
    "    score_matrix = np.zeros((n + 1, m + 1))\n",
    "    traceback_matrix = np.zeros((n + 1, m + 1), dtype='object')\n",
    "\n",
    "    # Initialize first row and column\n",
    "    for i in range(1, n + 1):\n",
    "        score_matrix[i][0] = gap_penalty * i\n",
    "        traceback_matrix[i][0] = 'up'  # Deletion\n",
    "    for j in range(1, m + 1):\n",
    "        score_matrix[0][j] = gap_penalty * j\n",
    "        traceback_matrix[0][j] = 'left'  # Insertion\n",
    "\n",
    "    # Fill in the scoring matrix\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            word1 = conll_words[i - 1].lower()\n",
    "            word2 = textgrid_words[j - 1].lower()\n",
    "\n",
    "            if word1 == word2:\n",
    "                score = match_score\n",
    "            else:\n",
    "                score = mismatch_penalty\n",
    "\n",
    "            diag_score = score_matrix[i - 1][j - 1] + score  # Match/Mismatch\n",
    "            up_score = score_matrix[i - 1][j] + gap_penalty   # Deletion\n",
    "            left_score = score_matrix[i][j - 1] + gap_penalty # Insertion\n",
    "\n",
    "            max_score = max(diag_score, up_score, left_score)\n",
    "            score_matrix[i][j] = max_score\n",
    "\n",
    "            # Traceback pointers\n",
    "            if max_score == diag_score:\n",
    "                traceback_matrix[i][j] = 'diag'\n",
    "            elif max_score == up_score:\n",
    "                traceback_matrix[i][j] = 'up'\n",
    "            else:\n",
    "                traceback_matrix[i][j] = 'left'\n",
    "\n",
    "    # Traceback to get the alignment\n",
    "    aligned_conll = []\n",
    "    aligned_textgrid = []\n",
    "    aligned_times = []\n",
    "    i, j = n, m\n",
    "\n",
    "    while i > 0 or j > 0:\n",
    "        direction = traceback_matrix[i][j]\n",
    "\n",
    "        if direction == 'diag':\n",
    "            aligned_conll.append(conll_words[i - 1])\n",
    "            aligned_textgrid.append(textgrid_words[j - 1])\n",
    "            aligned_times.append(textgrid_times[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif direction == 'up':\n",
    "            aligned_conll.append(conll_words[i - 1])\n",
    "            aligned_textgrid.append('-')  # Gap in TextGrid\n",
    "            aligned_times.append((None, None))\n",
    "            i -= 1\n",
    "        else:  # 'left'\n",
    "            aligned_conll.append('-')  # Gap in CoNLL\n",
    "            aligned_textgrid.append(textgrid_words[j - 1])\n",
    "            aligned_times.append(textgrid_times[j - 1])\n",
    "            j -= 1\n",
    "\n",
    "    # Reverse the aligned sequences\n",
    "    aligned_conll = aligned_conll[::-1]\n",
    "    aligned_textgrid = aligned_textgrid[::-1]\n",
    "    aligned_times = aligned_times[::-1]\n",
    "\n",
    "    return aligned_conll, aligned_textgrid, aligned_times\n",
    "\n",
    "\n",
    "\n",
    "def add_timestamps_to_conll(conll_path, textgrid_intervals, output_path):\n",
    "    \"\"\"Add timestamps from TextGrid intervals to a CoNLL file and create alignment report.\"\"\"\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    # Define output paths\n",
    "    output_dir = os.path.dirname(output_path)\n",
    "    base_name = os.path.splitext(os.path.basename(output_path))[0]\n",
    "    alignment_report_path = os.path.join(output_dir, f\"{base_name}_alignment_report.txt\")\n",
    "\n",
    "    # Read CoNLL file\n",
    "    column_names = ['col0', 'col1', 'col2', 'word', 'col4', 'col5', 'col6', \n",
    "                   'col7', 'col8', 'col9', 'col10', 'col11', \"col12\"]\n",
    "    \n",
    "    conll_data = pd.read_csv(\n",
    "        conll_path,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=column_names,\n",
    "        skip_blank_lines=True,\n",
    "    )\n",
    "\n",
    "    # Extract words and align\n",
    "    conll_words = conll_data['word'].astype(str).tolist()\n",
    "    aligned_conll, aligned_textgrid, aligned_times = align_sequences(conll_words, textgrid_intervals)\n",
    "\n",
    "    # Prepare alignment report\n",
    "    alignment_issues = []\n",
    "    for idx, (c_word, tg_word) in enumerate(zip(aligned_conll, aligned_textgrid)):\n",
    "        if c_word != tg_word and c_word != '-' and tg_word != '-':\n",
    "            alignment_issues.append(\n",
    "                f\"Mismatch at position {idx}: CoNLL word '{c_word}' vs. TextGrid word '{tg_word}'\"\n",
    "            )\n",
    "        elif c_word == '-' or tg_word == '-':\n",
    "            alignment_issues.append(\n",
    "                f\"Insertion/Deletion at position {idx}: CoNLL word '{c_word}' vs. TextGrid word '{tg_word}'\"\n",
    "            )\n",
    "\n",
    "    # Write alignment report\n",
    "    with open(alignment_report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"Alignment Report\\n\")\n",
    "        f.write(\"================\\n\\n\")\n",
    "        f.write(f\"Total words in CoNLL: {len(conll_words)}\\n\")\n",
    "        f.write(f\"Total words in TextGrid: {len(textgrid_intervals)}\\n\")\n",
    "        f.write(f\"Total alignment issues: {len(alignment_issues)}\\n\\n\")\n",
    "        f.write(\"Detailed Issues:\\n\")\n",
    "        f.write(\"---------------\\n\")\n",
    "        for issue in alignment_issues:\n",
    "            f.write(f\"{issue}\\n\")\n",
    "\n",
    "    # Prepare timestamps for CoNLL file\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    for conll_word, times in zip(aligned_conll, aligned_times):\n",
    "        if conll_word != '-':\n",
    "            start_times.append(times[0])\n",
    "            end_times.append(times[1])\n",
    "\n",
    "    # Add timestamps to DataFrame\n",
    "    conll_data = conll_data.loc[conll_data['word'] != '-'].reset_index(drop=True)\n",
    "    conll_data['start_time'] = start_times\n",
    "    conll_data['end_time'] = end_times\n",
    "\n",
    "    # Save timestamped CoNLL\n",
    "    conll_data.to_csv(output_path, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "    return output_path, alignment_report_path\n",
    "\n",
    "def process_files(input_dir, output_dir=None):\n",
    "    \"\"\"Setup directory structure and return core file paths.\"\"\"\n",
    "    input_dir = os.path.abspath(input_dir)\n",
    "    output_dir = os.path.abspath(output_dir) if output_dir else input_dir\n",
    "\n",
    "    if not os.path.isdir(input_dir):\n",
    "        raise ValueError(f\"Input directory does not exist: {input_dir}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Find core input files\n",
    "    conll_files = [f for f in os.listdir(input_dir) if f.endswith('.conll')]\n",
    "    if not conll_files:\n",
    "        raise FileNotFoundError(\"No .conll file found in input directory\")\n",
    "    \n",
    "    base_name = os.path.splitext(conll_files[0])[0]\n",
    "    conll_file = os.path.join(input_dir, conll_files[0])\n",
    "    audio_file = os.path.join(input_dir, f\"{base_name}.wav\")\n",
    "\n",
    "    if not os.path.isfile(audio_file):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_file}\")\n",
    "\n",
    "    return {\n",
    "        'input_dir': input_dir,\n",
    "        'output_dir': output_dir,\n",
    "        'base_name': base_name,\n",
    "        'conll': conll_file,\n",
    "        'audio': audio_file\n",
    "    }\n",
    "\n",
    "def verify_mfa_alignment_mms(audio_file, intervals):\n",
    "\n",
    "\n",
    "    # Load the processor and model\n",
    "    processor = AutoProcessor.from_pretrained(\"mms-meta/mms-zeroshot-300m\")\n",
    "    model = AutoModelForCTC.from_pretrained(\"mms-meta/mms-zeroshot-300m\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load the audio file\n",
    "    speech_array, sampling_rate = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "    results = []\n",
    "    print(\"Starting MMS...\")\n",
    "    length = len(intervals)\n",
    "    for index, interval in enumerate(intervals):\n",
    "        start_time = interval['start_time']\n",
    "        end_time = interval['end_time']\n",
    "        expected_text = interval['word']\n",
    "\n",
    "        # Extract the audio segment\n",
    "        start_sample = int(start_time * sampling_rate)\n",
    "        end_sample = int(end_time * sampling_rate)\n",
    "        audio_segment = speech_array[start_sample:end_sample]\n",
    "\n",
    "        # Prepare input values\n",
    "        inputs = processor(audio_segment, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "\n",
    "        # Get logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "\n",
    "        # Decode the predicted ids to text\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.decode(predicted_ids[0])\n",
    "\n",
    "        # Append the result\n",
    "        results.append({\n",
    "            'start_time': start_time,\n",
    "            'end_time': end_time,\n",
    "            'expected_text': expected_text,\n",
    "            'transcribed_text': transcription\n",
    "        })\n",
    "        progress = (index / length) * 100\n",
    "        print(f\"{index}/{length} ; {progress:.2f}%\")\n",
    "\n",
    "    # Convert results to a DataFrame\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"MMS Done!\")\n",
    "    return df_results\n",
    "\n",
    "\n",
    "\n",
    "def run_gentle_align(audio_file, transcript_file, output_dir, base_name):\n",
    "    \"\"\"Run Gentle aligner using Gentle server via Docker.\"\"\"\n",
    "    \n",
    "    gentle_json = os.path.join(output_dir, f\"{base_name}_gentle.json\")\n",
    "\n",
    "    # Verify files exist\n",
    "    if not os.path.exists(audio_file) or not os.path.exists(transcript_file):\n",
    "        print(\"Error: Input files not found\")\n",
    "        return None\n",
    "\n",
    "    # Prepare request\n",
    "    url = \"http://localhost:8765/transcriptions?async=false\"\n",
    "\n",
    "    files = {\n",
    "        'audio': open(audio_file, 'rb'),\n",
    "        'transcript': open(transcript_file, 'rb')\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"Sending files to Gentle server for alignment.\")\n",
    "        response = requests.post(url, files=files)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # Save output\n",
    "        with open(gentle_json, 'w', encoding='utf-8') as f:\n",
    "            json.dump(response.json(), f, indent=2)\n",
    "            \n",
    "        return gentle_json\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Gentle server error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def merge_aligners_data(output_conll, gentle_output, output_dir, base_name):\n",
    "    \"\"\"\n",
    "    Merge MFA and Gentle alignment data using MFA as baseline.\n",
    "    \n",
    "    Args:\n",
    "        output_conll (str): Path to MFA output conll file with timestamps\n",
    "        gentle_output (str): Path to Gentle JSON output\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Contains columns:\n",
    "            - word: The word\n",
    "            - mfa_start_time: Start time from MFA\n",
    "            - mfa_end_time: End time from MFA  \n",
    "            - gentle_start_time: Start time from Gentle (if matched)\n",
    "            - gentle_end_time: End time from Gentle (if matched)\n",
    "    \"\"\"\n",
    "    merged_csv = os.path.join(output_dir, f\"{base_name}_merged.csv\")\n",
    "    # Parse MFA conll file\n",
    "    mfa_words = []\n",
    "    with open(output_conll, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 15:  # Check if line has timestamps\n",
    "                continue\n",
    "            word = parts[3]\n",
    "            start_time = parts[13]\n",
    "            end_time = parts[14]\n",
    "            \n",
    "            if start_time and end_time:  # Only add if timestamps exist\n",
    "                mfa_words.append({\n",
    "                    'word': word.lower(),\n",
    "                    'mfa_start_time': float(start_time),\n",
    "                    'mfa_end_time': float(end_time)\n",
    "                })\n",
    "    \n",
    "    # Create MFA DataFrame\n",
    "    mfa_df = pd.DataFrame(mfa_words)\n",
    "    \n",
    "    # Parse Gentle JSON\n",
    "    with open(gentle_output, 'r', encoding='utf-8') as f:\n",
    "        gentle_data = json.load(f)\n",
    "    \n",
    "    gentle_words = []\n",
    "    for word in gentle_data.get('words', []):\n",
    "        if word.get('case') == 'success':\n",
    "            gentle_words.append({\n",
    "                'word': word.get('alignedWord', '').lower(),\n",
    "                'gentle_start_time': word.get('start'),\n",
    "                'gentle_end_time': word.get('end')\n",
    "            })\n",
    "    \n",
    "    gentle_df = pd.DataFrame(gentle_words)\n",
    "    \n",
    "    # Add empty columns for gentle timestamps\n",
    "    mfa_df['gentle_start_time'] = None\n",
    "    mfa_df['gentle_end_time'] = None\n",
    "    \n",
    "    # Match gentle words to MFA words\n",
    "    for idx, mfa_row in enumerate(mfa_df.itertuples()):\n",
    "        gentle_match = gentle_df[gentle_df['word'] == mfa_row.word]\n",
    "        \n",
    "        if not gentle_match.empty:\n",
    "            mfa_df.at[idx, 'gentle_start_time'] = gentle_match.iloc[0]['gentle_start_time']\n",
    "            mfa_df.at[idx, 'gentle_end_time'] = gentle_match.iloc[0]['gentle_end_time']\n",
    "    \n",
    "    analysis = analyze_alignment_differences(mfa_df)\n",
    "    \n",
    "    mfa_df.to_csv(merged_csv, index=False)\n",
    "    analysis_file = os.path.join(output_dir, f\"{base_name}_alignment_analysis.json\")\n",
    "    with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "        # Convert DataFrame to dict for JSON serialization\n",
    "        analysis['unmatched'] = analysis['unmatched'].to_dict(orient='records')\n",
    "        json.dump(analysis, f, indent=2)\n",
    "    \n",
    "    return merged_csv, analysis_file\n",
    "\n",
    "\n",
    "def analyze_alignment_differences(df):\n",
    "    \"\"\"Analyze differences between MFA and Gentle alignments.\"\"\"\n",
    "    # Get matched entries\n",
    "    matched_df = df.dropna(subset=['gentle_start_time', 'gentle_end_time'])\n",
    "    \n",
    "    # Calculate differences for matched entries\n",
    "    start_diffs = matched_df['gentle_start_time'] - matched_df['mfa_start_time']\n",
    "    end_diffs = matched_df['gentle_end_time'] - matched_df['mfa_end_time']\n",
    "    \n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'start_time': {\n",
    "            'mean_diff': start_diffs.mean(),\n",
    "            'std_diff': start_diffs.std(),\n",
    "            'max_diff': start_diffs.max(),\n",
    "            'min_diff': start_diffs.min()\n",
    "        },\n",
    "        'end_time': {\n",
    "            'mean_diff': end_diffs.mean(),\n",
    "            'std_diff': end_diffs.std(),\n",
    "            'max_diff': end_diffs.max(),\n",
    "            'min_diff': end_diffs.min()\n",
    "        },\n",
    "        'total_words': len(df),\n",
    "        'matched_words': len(matched_df),\n",
    "        'match_rate': len(matched_df) / len(df) * 100\n",
    "    }\n",
    "    \n",
    "    # Get unmatched entries\n",
    "    unmatched_df = df[df['gentle_start_time'].isna()].copy()\n",
    "    unmatched_df['position'] = range(len(unmatched_df))\n",
    "    \n",
    "    # Create matched words analysis\n",
    "    matched_words_analysis = matched_df.apply(\n",
    "        lambda row: {\n",
    "            'word': row['word'],\n",
    "            'position': row.name,\n",
    "            'mfa_start': row['mfa_start_time'],\n",
    "            'mfa_end': row['mfa_end_time'],\n",
    "            'gentle_start': row['gentle_start_time'],\n",
    "            'gentle_end': row['gentle_end_time'],\n",
    "            'start_diff': row['gentle_start_time'] - row['mfa_start_time'],\n",
    "            'end_diff': row['gentle_end_time'] - row['mfa_end_time']\n",
    "        }, axis=1).tolist()\n",
    "    \n",
    "    return {\n",
    "        'stats': stats,\n",
    "        'unmatched': unmatched_df[['position', 'word', 'mfa_start_time', 'mfa_end_time']],\n",
    "        'matched_words': matched_words_analysis\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing files: Input directory does not exist: C:\\Users\\manto\\Documents\\Coreference\\test3\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"C:/Users/manto/Documents/Coreference/tes\"\n",
    "output_dir = \"\"\n",
    "\n",
    "if (output_dir == \"\"):\n",
    "    output_dir = input_dir\n",
    "\n",
    "try:\n",
    "    # Get core paths\n",
    "    paths = process_files(input_dir, output_dir)\n",
    "    \n",
    "    # Convert and get txt path\n",
    "    txt_file = convert_conll_to_txt(paths['conll'], paths['output_dir'], paths['base_name'])\n",
    "    \n",
    "    # Run MFA\n",
    "    run_mfa_align(paths['input_dir'], paths['output_dir'])\n",
    "    \n",
    "    # Get TextGrid path and extract intervals\n",
    "    textgrid_file = os.path.join(paths['output_dir'], f\"{paths['base_name']}.TextGrid\")\n",
    "    intervals = extract_textgrid_intervals(textgrid_file)\n",
    "    \n",
    "    # Add timestamps to new CoNLL\n",
    "    output_conll = os.path.join(paths['output_dir'], f\"{paths['base_name']}_time.conll\")\n",
    "    add_timestamps_to_conll(paths['conll'], intervals, output_conll)\n",
    "    \n",
    "    # Run Gentle\n",
    "    gentle_json = run_gentle_align(paths['audio'], txt_file, paths['output_dir'], paths['base_name'])\n",
    "    \n",
    "    # Merge alignments\n",
    "    if gentle_json:\n",
    "        merged_csv, analysis_file = merge_aligners_data(output_conll, gentle_json, paths['output_dir'], paths['base_name'])\n",
    "        #mmsFrame = verify_mfa_alignment_mms(audio_file, intervals)\n",
    "        #mmsFrame.to_csv(\"results-mms.csv\", sep = '\\t')\n",
    "        #whisperFrame = verify_mfa_alignment_whisper(audio_file, intervals, \"tiny\") #Verficiation using Whisper Returns a pandaframe\n",
    "        #whisperFrame.to_csv(\"results.csv\", sep='\\t')\n",
    "\n",
    "        print(f\"Successfully processed files in {input_dir}.Output files saved in: {output_dir} \")\n",
    "except Exception as e:\n",
    "        print(f\"Error processing files: {e}\")\n",
    "        #print(mmsFrame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aligner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
